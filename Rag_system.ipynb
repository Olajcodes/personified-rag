{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c880f96",
   "metadata": {},
   "source": [
    "**Importing necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a81e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"All Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac120d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the API Keys\n",
    "load_dotenv()\n",
    "grok_key = os.getenv(\"GROK_API_KEY\")\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify API key\n",
    "if not (grok_key or google_key or openai_key):\n",
    "    print(\"WARNING: No LLM API Keys found (Grok/Google/OpenAI). Chat generation might fail.\")\n",
    "print(f\"API Keys Loaded. Grok: {bool(grok_key)}, Google: {bool(google_key)}, OpenAI: {bool(openai_key)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89414",
   "metadata": {},
   "source": [
    "**Data Ingestion from GitHub and Linkedin**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d33578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration --- #\n",
    "GITHUB_REPO_URL = \"https://github.com/Olajcodes/Olajcodes\" \n",
    "LINKEDIN_PDF_PATH = \"Profile.pdf\" \n",
    "\n",
    "documents = []\n",
    "\n",
    "# 1. Load GitHub Data using Clone method\n",
    "try:\n",
    "    print(f\"Loading GitHub repository from {GITHUB_REPO_URL}...\")\n",
    "    loader_github = GitLoader(\n",
    "        clone_url=GITHUB_REPO_URL,\n",
    "        repo_path=\"./temp_repo\",\n",
    "        branch=\"main\",\n",
    "        file_filter=lambda file_path: file_path.endswith((\".md\", \".py\", \".js\", \".ts\", \".html\", \".ipynb\")) # Adjust filters\n",
    "    )\n",
    "    github_docs = loader_github.load()\n",
    "    documents.extend(github_docs)\n",
    "    print(f\"Loaded {len(github_docs)} documents from GitHub.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GitHub: {e}\")\n",
    "\n",
    "# 2. Load LinkedIn Data\n",
    "try:\n",
    "    print(f\"Loading LinkedIn profile from {LINKEDIN_PDF_PATH}...\")\n",
    "    loader_linkedin = PyPDFLoader(LINKEDIN_PDF_PATH)\n",
    "    linkedin_docs = loader_linkedin.load()\n",
    "    documents.extend(linkedin_docs)\n",
    "    print(f\"Loaded {len(linkedin_docs)} pages from LinkedIn.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LinkedIn (Ensure file exists): {e}\")\n",
    "\n",
    "# 3. Split Text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total split documents ready for embedding: {len(splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6af52",
   "metadata": {},
   "source": [
    "**Vector Store (ChromaDB)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIRECTORY = \"./chroma_db\"\n",
    "\n",
    "# Embedding Selection Logic - LOCAL HUGGINGFACE\n",
    "print(\"Using Local HuggingFace Embeddings (all-MiniLM-L6-v2)\")\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Chroma and persist data\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")\n",
    "\n",
    "print(f\"Embeddings generated and persisted to {PERSIST_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf578c",
   "metadata": {},
   "source": [
    "**RAG Pipeline with LCEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9404df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (Priority: Gemini -> Grok -> OpenAI)\n",
    "if google_key:\n",
    "    print(\"Using Gemini\")\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=google_key, temperature=0)\n",
    "elif grok_key:\n",
    "    print(\"Using Grok (via xAI)\")\n",
    "    llm = ChatOpenAI(model=\"grok-beta\", base_url=\"https://api.x.ai/v1\", api_key=grok_key, temperature=0)\n",
    "elif openai_key:\n",
    "    print(\"Using OpenAI\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", api_key=openai_key, temperature=0)\n",
    "else:\n",
    "    raise ValueError(\"No LLM Available\")\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# --- Privacy & System Prompt ---\n",
    "system_prompt_text = \"\"\"\n",
    "You are a professional assistant representing a developer. Your knowledge is based STRICTLY on the provided context (GitHub repositories and LinkedIn profile).\n",
    "\n",
    "### INSTRUCTIONS:\n",
    "1. Answer questions about professional experience, skills, repositories, and technical implementation details.\n",
    "2. If the context does not contain the answer, say \"I don't have that information in my knowledge base.\"\n",
    "3. ALWAYS cite your sources implicitly by referring to the specific file or section.\n",
    "4. Format all responses as clean plain text with no markdown or special characters.\n",
    "\n",
    "### PRIVACY GUARDRAILS (CRITICAL):\n",
    "You MUST REFUSE to answer questions about the following personal sensitive information, even if it might be present in the context:\n",
    "- Age\n",
    "- Date of birth\n",
    "- Home Address\n",
    "- Phone number\n",
    "- Personal Email address\n",
    "- Any other sensitive personal identifiers\n",
    "\n",
    "If a user asks for this information, reply EXACTLY with:\n",
    "\"I cannot share personal or sensitive information such as contact details or age. Please ask about his professional experience or projects.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_text),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Format docs for the prompt\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain using LCEL\n",
    "from operator import itemgetter\n",
    "rag_chain = (\n",
    "    {\n",
    "        # Extract the question string specifically for the retriever\n",
    "        \"context\": itemgetter(\"question\") | retriever | format_docs, \n",
    "        \n",
    "        # Pass the other keys through as needed\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG Chain initialized and Created with Privacy Guardrails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72de129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the chain\n",
    "# Create an empty list for history if this is the first turn\n",
    "chat_history = [] \n",
    "\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "try:\n",
    "    # Note: get_openai_callback might not track Gemini tokens accurately, but won't crash\n",
    "    with get_openai_callback() as cb:\n",
    "        response = rag_chain.invoke({\n",
    "            \"question\": \"How old is He?\",\n",
    "            \"chat_history\": chat_history\n",
    "        })\n",
    "        print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "        print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae5d49",
   "metadata": {},
   "source": [
    "**CLI Interactive Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "print(\"--- Conversational RAG System Online ---\")\n",
    "print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"User: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    # Invoke Chain\n",
    "    response = rag_chain.invoke({\n",
    "        \"question\": \"What is Stack is Olajcodes?\",\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    \n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "    \n",
    "    # Update History\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=query),\n",
    "        AIMessage(content=response)\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
